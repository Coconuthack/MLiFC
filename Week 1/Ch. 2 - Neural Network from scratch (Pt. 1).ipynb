{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural net from scratch\n",
    "This notebook aims to show students the inner workings of a basic neural network while at the same time iluminating some theoretical foundations that underpin machine learning in general. This work is inspired by [Trask's 'A neural network in 11 lines of Python'](https://iamtrask.github.io/2015/07/12/basic-python-network/).\n",
    "\n",
    "## Maths\n",
    "This exercise is quite mathematical and technical, but let me justify why I think knowing all this maths is important. Machine learning algorithms and neural networks in particular can seem a lot like magic. Because they are black boxes, that is we do not know how exactly they arrive at their results, the computations are too large for any human to follow, and the field has made such fast advances, many people have an unrealistic understanding of the technology. It seems to them like the computer is now genuinely intelligent and the rise of the robots is near. I have found that people who worked through the underlying math of a neural network gain a much more nuanced understanding and recognize its abilities and limitations much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN architecture\n",
    "In this exercise, we will be working on a supervised learning problem. This means we got an input matrix $X$ and an output vector $y$ and we want to train our algorithm on making predictions for $y$. Let's give the data a look:\n",
    "\n",
    "|$X_1$|$X_2$|$X_3$|$y$|\n",
    "|-|-|-|---|\n",
    "|0|1|0|0|\n",
    "|1|0|0|1|\n",
    "|1|1|1|1|\n",
    "|0|1|1|0|\n",
    "\n",
    "Our data has three input features, $X_1$,$X_2$ and $X_3$ and one output, $y$ with all values being either 1 or 0. You can also see that we have four examples in our data and that we combined the data into one matrix by stacking horizontal vectors. You might have noticed that $X_1$ is perfectly correlated with $y$. Finding those correlations automatically is exactly the task of our neural net. In this exercise we will implement a two layer network, also known as a perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A perceptron\n",
    "\n",
    "![Perceptron](./assets/Perceptron_moj.png)\n",
    "TODO: Simplified image with sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear forward\n",
    "As you can see in the image above, our neural net will consist of only one input layer and one output layer. The first the input features are multiplied with the weights $w$ and the weighted features then get summed up, together with the bias b. This is exactly what happens in [linear regression](https://en.wikipedia.org/wiki/Linear_regression), a technique you might be familiar with. In linear regression, the bias $b$ is usually denoted as $\\beta_0$ while the weights are named $\\beta_1, \\beta_2, \\beta_3...$ For convenience we are just calling $\\beta_0$ $b$ and group all other weights into the weight vector $w$. $w$ and $b$ are called the parameters of the neural network. The outcome of our linear regression gets then passed into a non linear activation function, in our case the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function). This is the same as what happens in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). \n",
    "\n",
    "$$ z_0 = Xw_0 + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sigmoid activation function\n",
    "\n",
    "If we had a linear activation function, the output would simply be a weighted sum of our input features. Through non-linear activation functions we can model much more complex functions. In fact, it has been shown that neural nets can model _any_ function, as long as we make them big enough. Our sigmoid function has the formula:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "And looks like this:\n",
    "![Sigmoid function](/assets/Logistic-curve.svg)\n",
    "\n",
    "We can define the Python function:\n",
    "```python\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights\n",
    "\n",
    "Now let's go backward through our perceptron. Before we can work with the weights, we have to initialize them.\n",
    "```python\n",
    "# initialize weights randomly with mean 0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "```\n",
    "This creates a 3 by 1 weight matrix, mapping our three input features to our one output. It is important to initialize the weights randomly and not just set them all to zero for the sake of symmetry breaking. If all the weights where the same, then they would never be different from each other, and our neural net could not model any complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full forward pass\n",
    "\n",
    "With all the elements in place, we can now obtain predictions from our network with a forward pass.\n",
    "\n",
    "1. We do a linear step:\n",
    "$$ z_0 = Xw_0 + b$$\n",
    "2. We pass the linear product through the activation function\n",
    "$$A_1 = \\sigma(z_0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60841366]\n",
      " [ 0.45860596]\n",
      " [ 0.3262757 ]\n",
      " [ 0.36375058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Seed the random function to ensure that we always get the same result\n",
    "np.random.seed(1)\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "#set up w0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "#define X\n",
    "X = np.array([[0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,1,1]])\n",
    "\n",
    "#define y\n",
    "y = np.array([[0,1,1,0]])\n",
    "#do the linear step\n",
    "z0 = np.dot(X,w0)\n",
    "#pass the linear step through the activation function\n",
    "A1 = sigmoid(z0)\n",
    "#see what we got\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring losses\n",
    " \n",
    " That does not look like $y$ at all! That is just random numbers! We now have to modify the weights so that we arrive at better predictions.\n",
    " In order to arrive at better predictions, we first have to quantify how badly we did. In classification the metric used is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy), sometimes also called logistic loss or log loss. It is calculated as follows:\n",
    " \n",
    " $$L(w) = -\\frac{1}{m} \\sum_{i=0}^m [y_i * \\log \\hat y_i  +  (1-y_i)*\\log(1-\\hat y_i)]$$\n",
    " \n",
    " Let's go through this step by step.\n",
    " \n",
    " 1. $L(w)$ is the loss function given the weights $w$ that where used to obtain the prediction $\\hat y_i$\n",
    " 2. $-\\frac{1}{m} \\sum_{i=0}^m$ The loss over a batch of m examples is the average loss of all examples. We have factored a - out of the sum and moved it in front of the equation.\n",
    " 3. $y_i * \\log \\hat y_i$ This part of the loss only comes into play if the true value, $y_i$ is 1. If $y_i$ is 1, we want $\\hat y_i$ to be as close to 1 as possible, to achieve a low loss.\n",
    " 4. $(1-y_i)*\\log(1-\\hat y_i)$ This part of the loss comes into play if $y_i$ is 0. If so, we want $\\hat y_i$ to be close to 0 as well.\n",
    " \n",
    " Consider the following example: We have only one sample so, \n",
    " $m = 1$, the true value $y_i = 1$ and the the prediction $\\hat y_i = 0.01$. $\\hat y_i$ can be interpreted as the probability that $y_i = 1$ given $X_i$, or $P(y_i = 1 | X_i)$ in math terms. It will never be zero since the probability of $y_i = 1$ will never be zero, only very small. Here, our algorithm has given $y_i = 1$ only a 1% chance. In the case above our loss can be calculated as: \n",
    " \n",
    " $$L(w) = -\\frac{1}{1} \\sum_{i=0}^1 [1 * \\log (0.01)  +  (1-1)*\\log(1-0)]$$\n",
    "  $$L(w) = -[-4.6051  +  0]$$\n",
    "  $$L(w) = 4.6051$$\n",
    "  \n",
    "  This is quite a high loss, let's see what would have happened if $\\hat y_i = 0.99$, meaning the algorithm would have given $y_i = 1$ a 99% chance.\n",
    "  \n",
    "  $$L(w) = -\\frac{1}{1} \\sum_{i=0}^1 [1 * \\log (0.99)  +  (1-1)*\\log(1-0.99)]$$\n",
    "   $$L(w) = -[-0.01  +  0]$$\n",
    "   $$L(w) = 0.01$$\n",
    "   \n",
    "Much better! As an exercise, calculate what the loss would be if the algorithm had $\\hat y_i = 0.99$ while $y_i = 0$. In the next section we will see how our algorithm can arrive at lower losses. Through a method called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    " \n",
    "![Gradient Descent](./assets/Gradient_descent.jpg)\n",
    " \n",
    "Now that we have a good measure for our loss, how do we decrease it. This is a classic minimization task. We want to find a value for $w$ that minimizes $L(w)$. In linear regression we know how to solve the loss function for a value of $w$ that minimizes the squared loss function. For neural networks, this is sadly not possible. But what we can do instead is follow the gradient through a method called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition\n",
    "Imagine you are on a mountain forrest on a hike. You have lost the track and are now in the woods trying to find home to the valley. Since there are so many trees, you can not see the valley, you can only see the ground under you. How would you find your way down? One sensible approach would be to follow the slope of the mountain. Where the slope goes downwards, you go. This is the same as a gradient descent algorithm does. The loss function is the mountain and to get to a low loss, the algorithm follows the slope, that is the derivative, of the loss function. When we walk down the mountain, we are updating our location coordinates. The algorithm updates the parameters of the neural network.\n",
    "\n",
    "![Stanford Gradient Descent Image](./assets/grad.png)\n",
    "\n",
    "As you can see in the image above, there is a potential problem of the algorithm converging in a local optima. That is, a point lower than all surrounding points, but not the lowest point in the loss function. In practice we find this not to be a huge concern. The parameter space is very high dimensional (there are many parameters to be updated), so a point being lower than _all_ surrounding points is quite rare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical representation\n",
    "Gradient descent updates a parameter by adding the negative gradient multiplied with the learning rate alpha. So for a parameter $a$, the update rule is:\n",
    "$$a := a - \\alpha * \\frac{dL(w)}{da}$$\n",
    "The symbol $:=$ denotes a value assignment. This is not an equation, therefore we do not use $=$. The learning rate $alpha$ is a so called hyper parameter, that ensure that the update is not too big and the algorithm does not 'jump over' the minimum. We will learn more about the learning rate and hyper parameters in the next chapter, so in this example I will just leave it out.\n",
    "\n",
    "To better understand why the parameters are updated with the _inverse_ of the gradient, take a look at this image.\n",
    "![Gradient Descent With Gradients](./assets/gradient_descent_trask.png)\n",
    "As you can see the gradient is positive when the slope is upwards facing. To get to a lower point on the cost function we have to go 'backwards'. The steeper the slope, the more we have to move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "Before we can start updating our weights, we have to calculate the derivative of course. In this case, we want the derivative of the total loss $L(w)$ with respect to the weight vector $w$. To get there, we have to use the chain rule:\n",
    "\n",
    "![Chain rule](./assets/forward_backward_pass.jpg)\n",
    "\n",
    "Calculating the derivative through what is called a computational graph was a big problem for a long time so it can be surprising that the solution is essentially the humble chain rule from high school. If you want to learn more about this topic I recommend [colah's blog post](http://colah.github.io/posts/2015-08-Backprop/).\n",
    "\n",
    "To keep things easy in this class I will simplify the derivatives and let $da$ denote the derivative of $L(w)$ with respect to $a$, whatever it may be.\n",
    "\n",
    "In code, the backpropagation and gradient descent update looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Calculate derivative of L(w) with respect to z0\n",
    "dz0 = log_loss_derivative(y=y,y_hat=A1)\n",
    "\n",
    "#Calculate derivative of L(w) with respect to w0\n",
    "dw0 = 1/m * np.dot(X.T,dz0)\n",
    "\n",
    "#Calculate derivative of L(w) with respect to b0\n",
    "db0 = 1/m * np.sum(dz0,axis=0,keepdims=True)\n",
    "\n",
    "#Update w0 accoarding to gradient descent algorithm\n",
    "w0 = w0 - dw0\n",
    "\n",
    "#Update b0 accoarding to gradient descent algorithm\n",
    "b0 = b0 - db0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimization algorithm\n",
    "Now, let's put all of this together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Seed the random function to ensure that we always get the same result\n",
    "np.random.seed(1)\n",
    "#Variable definition\n",
    "#set up w0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "#define X\n",
    "X = np.array([[0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,1,1]])\n",
    "#define y\n",
    "y = np.array([[0,1,1,0]])\n",
    "\n",
    "#b may be 0\n",
    "b0 = 0\n",
    "m = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup steps\n",
    "import numpy as np\n",
    "#Seed the random function to ensure that we always get the same result\n",
    "np.random.seed(1)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#Log Loss function\n",
    "def log_loss(y,y_hat):\n",
    "    N = y.shape[0]\n",
    "    l = -1/N * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "    return l\n",
    "\n",
    "def log_loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Loss: 11.6120393773\n",
      "Episode: 1  Loss: 5.22014800909\n",
      "Episode: 2  Loss: 3.32253894866\n",
      "Episode: 3  Loss: 2.45427185654\n",
      "Episode: 4  Loss: 1.95188288684\n",
      "Episode: 5  Loss: 1.62279382592\n",
      "Episode: 6  Loss: 1.38996107476\n",
      "Episode: 7  Loss: 1.21629872373\n",
      "Episode: 8  Loss: 1.08167437784\n",
      "Episode: 9  Loss: 0.974189434371\n",
      "Episode: 10  Loss: 0.886349950808\n",
      "Episode: 11  Loss: 0.813196552213\n",
      "Episode: 12  Loss: 0.751314198221\n",
      "Episode: 13  Loss: 0.698273298687\n",
      "Episode: 14  Loss: 0.652297519684\n",
      "Episode: 15  Loss: 0.612057670024\n",
      "Episode: 16  Loss: 0.576539098591\n",
      "Episode: 17  Loss: 0.544953698518\n",
      "Episode: 18  Loss: 0.516679923408\n",
      "Episode: 19  Loss: 0.491220926294\n",
      "Episode: 20  Loss: 0.468174733682\n",
      "Episode: 21  Loss: 0.447212598879\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for i in range(22):\n",
    "    #do the linear step\n",
    "    z0 = np.dot(X,w0) + b0\n",
    "    \n",
    "    #pass the linear step through the activation function\n",
    "    A1 = sigmoid(z0)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = log_loss(y=y,y_hat=A1)\n",
    "    print('Episode:',i,' Loss:',loss)\n",
    "    #we use this to keep track of losses over time\n",
    "    losses.append(loss)\n",
    "    \n",
    "    #Calculate derivative of L(w) with respect to z0\n",
    "    dz0 = log_loss_derivative(y=y,y_hat=A1)\n",
    "\n",
    "    #Calculate derivative of L(w) with respect to w0\n",
    "    dw0 = 1/m * np.dot(X.T,dz0)\n",
    "\n",
    "    #Calculate derivative of L(w) with respect to b0\n",
    "    db0 = 1/m * np.sum(dz0,axis=0,keepdims=True)\n",
    "\n",
    "    #Update w0 accoarding to gradient descent algorithm\n",
    "    #To keep things easy we will ignore the learning rate alpha for now, it will be covered in the next chapter\n",
    "    w0 = w0 - dw0\n",
    "\n",
    "    #Update b0 accoarding to gradient descent algorithm\n",
    "    b0 = b0 - db0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1086b84a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot losses over time\n",
    "#As you can see our algorithm does quite well and quickly reduces losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting words\n",
    "In this chapter you have built your first (very simple) neural network from scratch. You have seen some of the most important tools like backpropagation and gradient descent. In the next chapter we will build a bigger network with more layers. You will also learn about your first hyper parameter, the learning rate $\\alpha$.\n",
    "I hope you have learned some key lessons in this chapter:\n",
    "### Key take aways\n",
    "1. A NN makes predictions through matrix multiplication and activation functions.\n",
    "2. A NN learns by updating its parameters by inverse gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
