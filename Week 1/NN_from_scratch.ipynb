{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural net from scratch\n",
    "This notebook aims to show students the inner workings of a basic neural network while at the same time iluminating some theoretical foundations that underpin machine learning in general. This work is largely inspired by [Trask's 'A neural network in 11 lines of Python'](https://iamtrask.github.io/2015/07/12/basic-python-network/).\n",
    "\n",
    "## Maths\n",
    "This exercise is quite mathematical and technical, but let me justify why I think knowing all this maths is important. Machine learning algorithms and neural networks in particular can seem a lot like magic. Because they are black boxes, that is we do not know how exactly they arrive at their results, the computations are too large for any human to follow, and the field has made such fast advances, many people have an unrealistic understanding of the technology. It seems to them like the computer is now genuinely intelligent and the rise of the robots is near. I have found that people who worked through the underlying math of a neural network gain a much more nuanced understanding and recognize its abilities and limitations much better.\n",
    "\n",
    "## NN architecture\n",
    "In this exercise, we will be working on a supervised learning problem. This means we got an input matrix $X$ and an output vector $y$ and we want to train our algorithm on making predictions for $y$. Let's give the data a look:\n",
    "\n",
    "|$X_1$|$X_2$|$X_3$|$y$|\n",
    "|-|-|-|---|\n",
    "|0|1|0|0|\n",
    "|1|0|0|1|\n",
    "|1|1|1|1|\n",
    "|0|1|1|0|\n",
    "\n",
    "Our data has three input features, $X_1$,$X_2$ and $X_3$ and one output, $y$ with all values being either 1 or 0. You can also see that we have four examples in our data and that we combined the data into one matrix by stacking horizontal vectors. You might have noticed that $X_1$ is perfectly correlated with $y$. Finding those correlations automatically is exactly the task of our neural net. In this exercise we will implement a two layer network, also known as a perceptron.\n",
    "\n",
    "### A perceptron\n",
    "\n",
    "![Perceptron](/assets/Perceptron_moj.png)\n",
    "TODO: Simplified image with sigmoid activation function.\n",
    "\n",
    "### Linear forward\n",
    "As you can see in the image above, our neural net will consist of only one input layer and one output layer. The first the input features are multiplied with the weights $w$ and the weighted features then get summed up, together with the bias b. This is exactly what happens in [linear regression](https://en.wikipedia.org/wiki/Linear_regression), a technique you might be familiar with. In linear regression, the bias $b$ is usually denoted as $\\beta_0$ while the weights are named $\\beta_1, \\beta_2, \\beta_3...$ For convenience we are just calling $\\beta_0$ $b$ and group all other weights into the weight vector $w$. $w$ and $b$ are called the parameters of the neural network. The outcome of our linear regression gets then passed into a non linear activation function, in our case the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function). This is the same as what happens in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). \n",
    "\n",
    "$$ z_0 = Xw_0 + b$$\n",
    "\n",
    "### The sigmoid activation function\n",
    "\n",
    "If we had a linear activation function, the output would simply be a weighted sum of our input features. Through non-linear activation functions we can model much more complex functions. In fact, it has been shown that neural nets can model _any_ function, as long as we make them big enough. Our sigmoid function has the formula:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "And looks like this:\n",
    "![Sigmoid function](/assets/Logistic-curve.svg)\n",
    "\n",
    "We can define the Python function:\n",
    "```python\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "```\n",
    "\n",
    "### Initializing weights\n",
    "\n",
    "Now let's go backward through our perceptron. Before we can work with the weights, we have to initialize them.\n",
    "```python\n",
    "# initialize weights randomly with mean 0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "```\n",
    "This creates a 3 by 1 weight matrix, mapping our three input features to our one output. It is important to initialize the weights randomly and not just set them all to zero for the sake of symmetry breaking. If all the weights where the same, then they would never be different from each other, and our neural net could not model any complex functions.\n",
    "\n",
    "### A full forward pass\n",
    "\n",
    "With all the elements in place, we can now obtain predictions from our network with a forward pass.\n",
    "\n",
    "1. We do a linear step:\n",
    "$$ z_0 = Xw_0 + b$$\n",
    "2. We pass the linear product through the activation function\n",
    "$$A_1 = \\sigma(z_0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.60841366]\n",
      " [ 0.45860596]\n",
      " [ 0.3262757 ]\n",
      " [ 0.36375058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#Seed the random function to ensure that we always get the same result\n",
    "np.random.seed(1)\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "#set up w0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "#define X\n",
    "X = np.array([[0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,1,1]])\n",
    "\n",
    "#define y\n",
    "y = np.array([[0,1,1,0]])\n",
    "#do the linear step\n",
    "z0 = np.dot(X,w0)\n",
    "#pass the linear step through the activation function\n",
    "A1 = sigmoid(z0)\n",
    "#see what we got\n",
    "print(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring losses\n",
    " \n",
    " That does not look like $y$ at all! That is just random numbers! We now have to modify the weights so that we arrive at better predictions.\n",
    " In order to arrive at better predictions, we first have to quantify how badly we did. In classification the metric used is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy), sometimes also called logistic loss or log loss. It is calculated as follows:\n",
    " \n",
    " $$L(w) = -\\frac{1}{N} \\sum_{n=1}^N [y_n * \\log \\hat y_n  +  (1-y_n)*\\log(1-\\hat y_n)]$$\n",
    " \n",
    " Let's go through this step by step.\n",
    " \n",
    " 1. $L(w)$ is the loss function given the weights $w$ that where used to obtain the prediction $\\hat y_n$\n",
    " 2. $-\\frac{1}{N} \\sum_{n=1}^N$ The loss over a batch of N examples is the average loss of all examples. We have factored a - out of the sum and moved it in front of the equation.\n",
    " 3. $y_n * \\log \\hat y_n$ This part of the loss only comes into play if the true value, $y_n$ is 1. If $y_n$ is 1, we want $\\hat y_n$ to be as close to 1 as possible, to achieve a low loss.\n",
    " 4. $(1-y_n)*\\log(1-\\hat y_n)$ This part of the loss comes into play if $y_n$ is 0. If so, we want $\\hat y_n$ to be close to 0 as well.\n",
    " \n",
    " Consider the following example: We have only one sample so, \n",
    " $N = 1$, the true value $y_n = 1$ and the the prediction $\\hat y_n = 0.01$. $\\hat y_n$ can be interpreted as the probability that $y_n = 1$ given $X_n$, or $P(y_n = 1 | X_n)$ in math terms. It will never be zero since the probability of $y_n = 1$ will never be zero, only very small. Here, our algorithm has given $y_n = 1$ only a 1% chance. In the case above our loss can be calculated as: \n",
    " \n",
    " $$L(w) = -\\frac{1}{1} \\sum_{n=1}^N [1 * \\log (0.01)  +  (1-1)*\\log(1-0)]$$\n",
    "  $$L(w) = -[-4.6051  +  0]$$\n",
    "  $$L(w) = 4.6051$$\n",
    "  \n",
    "  This is quite a high loss, let's see what would have happened if $\\hat y_n = 0.99$, meaning the algorithm would have given $y_n = 1$ a 99% chance.\n",
    "  \n",
    "  $$L(w) = -\\frac{1}{1} \\sum_{n=1}^N [1 * \\log (0.99)  +  (1-1)*\\log(1-0.99)]$$\n",
    "   $$L(w) = -[-0.01  +  0]$$\n",
    "   $$L(w) = 0.01$$\n",
    "   \n",
    "Much better! As an exercise, calculate what the loss would be if the algorithm had $\\hat y_n = 0.99$ while $y_n = 0$. In the next section we will see how our algorithm can arrive at lower losses. Through a method called gradient descent.\n",
    " \n",
    " ## Gradient descent\n",
    " \n",
    " TODO: IMAGE HERE\n",
    " \n",
    " Now that we have a good measure for our loss, how do we decrease it. This is a classic minimization task. We want to find a value for $w$ that minimizes $L(w)$. In linear regression we know how to solve the loss function for a value of $w$ that minimizes the squared loss function. For neural networks, this is sadly not possible. But what we can do instead is follow the gradient through a method called gradient descent.\n",
    " \n",
    "### Intuition\n",
    "Imagine you are on a mountain forrest on a hike. You have lost the track and are now in the woods trying to find home to the valley. Since there are so many trees, you can not see the valley, you can only see the ground under you. How would you find your way down? One sensible approach would be to follow the slope of the mountain. Where the slope goes downwards, you go. This is the same as a gradient descent algorithm does. The loss function is the mountain and to get to a low loss, the algorithm follows the slope, that is the derivative, of the loss function. When we walk down the mountain, we are updating our location coordinates. The algorithm updates the parameters of the neural network.\n",
    "\n",
    "![Stanford Gradient Descent Image](./assets/grad.png)\n",
    "\n",
    "As you can see in the image above, there is a potential problem of the algorithm converging in a local optima. That is, a point lower than all surrounding points, but not the lowest point in the loss function. In practice we find this not to be a huge concern. The parameter space is very high dimensional (there are many parameters to be updated), so a point being lower than _all_ surrounding points is quite rare.\n",
    "\n",
    "### Backpropagation\n",
    "Before we can start updating our weights, we have to calculate the derivative of course. In this case, we want the derivative of the total loss $L(w)$ with respect to the weight vector $w$. To get there, we have to use the chain rule:\n",
    "\n",
    "![Chain rule](./assets/forward_backward_pass.png)\n",
    "\n",
    "Calculating the derivative through what is called a computational graph was a big problem for a long time so it can be surprising that the solution is essentially the humble chain rule from high school. If you want to learn more about this topic I recommend [colah's blog post](http://colah.github.io/posts/2015-08-Backprop/).\n",
    "\n",
    "To keep things easy in this class I will simplify the derivatives and let $da$ denote the derivative of $L(w)$ with respect to $a$, whatever it may be.\n",
    "\n",
    "### The optimization algorithm\n",
    "Now, let's put all of this together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup steps\n",
    "import numpy as np\n",
    "#Seed the random function to ensure that we always get the same result\n",
    "np.random.seed(1)\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#New sigmoid derivative function for backward pass\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "#Log Loss function\n",
    "def log_loss(y,y_hat):\n",
    "    N = y.shape[0]\n",
    "    l = -1/N * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "    return l\n",
    "\n",
    "def log_loss_derivative(y,y_hat):\n",
    "    return (y - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set up w0\n",
    "w0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "#define X\n",
    "X = np.array([[0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,1,1]])\n",
    "\n",
    "#define y\n",
    "y = np.array([[0,1,1,0]])\n",
    "\n",
    "#b may be 0\n",
    "b = 0\n",
    "\n",
    "#This part is the forward pass all over again\n",
    "#do the linear step\n",
    "z0 = np.dot(X,w0) + b\n",
    "#pass the linear step through the activation function\n",
    "A1 = sigmoid(z0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
